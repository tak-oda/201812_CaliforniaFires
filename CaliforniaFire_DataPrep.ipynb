{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Bot Behavior in California Fires -Data Preparation-\n",
    "### Takeshi Oda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. About this data analysis\n",
    "### 1.1. Twitter Bot\n",
    "Twitter Bot is a type of twitter user account which is programmed to post messages into twitter autonomously.   \n",
    "It is known that improper uses of twitter bots are causing harmful effect on public communication in social media since they are sometimes used to manipulate public opinion or confuse truth with fake news. \n",
    "\n",
    "### 1.2. California Fires\n",
    "The recent and worst wild fire in California ‘Campfire’ occurred on 8th November 2018 and killed around 85 people (as of 02/12/20018). \n",
    "\n",
    "### 1.3. My question\n",
    "My question in this project is:  \n",
    "\n",
    "**'Is there any differences in behavior between Bot and Human on Twitter in the field of natural disaster?'**     \n",
    "\n",
    "I guessed twitter was used to claim political or environmental opinion by Bots as well as expressing sadness or hope on events through this disaster. I would learn whether there is any different pattern of tweets between bots and normal users. Also, I would learn which type of accounts are more likely to post political or environmental mentioning.\n",
    "\n",
    "### 1.4. Approach\n",
    "Tweets about California Fires were collected and probability of being Bot is assigned to each tweet.  \n",
    "To assign the probability, I utilized an API **'Botometer'** which is provided by **Indiana University and the Center for Complex Networks and Systems Research (CNetS)**.\n",
    "\n",
    "Tweets were divided into two groups,i.e. Bot and Non-Bot in R and statistical testing was conducted on several metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Library settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy \n",
    "import botometer\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Collection \n",
    "\n",
    "### 3.1. Tweepy \n",
    "I made a plain text file called twitter_credentials.py, and put it into my home directory.  \n",
    "It contains credentials for Twitter API and Botometer API.\n",
    "\n",
    "    --- Credentials for Twitter API ---\n",
    "    consumer_key = '...'\n",
    "    consumer_secret = '...'\n",
    "    access_token = '...'\n",
    "    access_token_secret = '...'\n",
    "    --- User Key for Botometer API ---\n",
    "    mashape_key = \"...\"  \n",
    "\n",
    "### 3.2. Tweet collection for California Fires\n",
    "By using tweepy, I extracted 60004 tweets with the keyward '#CaliforniaFires'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ~/twitter_credentials.py\n",
    "\n",
    "## COllect Tweets by Keyword ##\n",
    "def collect_tweet(keyword, file, notweet=False):\n",
    "    \"\"\"\n",
    "    Collect tweet by keyword\n",
    "        \n",
    "    Parameters\n",
    "    --------------\n",
    "    keyword\n",
    "     Search key word such as hashtags, user name\n",
    "    file\n",
    "     file path to which returned tweet data is written\n",
    "    notweet\n",
    "     Set True if you do not want to receive text message from twitter.\n",
    "     Only date, retweet count, user name and screen name are returned in this case.\n",
    "        \n",
    "    Returns\n",
    "    --------------\n",
    "    None\n",
    "        \n",
    "    \"\"\"\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True) #wait for unlocking rate limit\n",
    "\n",
    "    user = api.me()\n",
    "    \n",
    "    numOfTweets = 60000 #Number of tweets to collect\n",
    "    #numOfTweets = 100\n",
    "    msglist = []\n",
    "    all_msg = []\n",
    "    \n",
    "    backoff_counter = 1\n",
    "    while True:\n",
    "        try:\n",
    "            for tweet in tweepy.Cursor(api.search, q=keyword, lang=\"en\", tweet_mode=\"extended\").items(numOfTweets):\n",
    "                \n",
    "                message = TextBlob(tweet.full_text)\n",
    "                message = message.strip()\n",
    "                user = tweet.user\n",
    "                user_name = user.name\n",
    "                screen_name = user.screen_name\n",
    "                if notweet == True:\n",
    "                    msglist.append( ( tweet.created_at, tweet.retweet_count, user_name, screen_name) )\n",
    "                else:\n",
    "                    msglist.append( ( tweet.created_at, tweet.retweet_count, user_name, screen_name, message ) )\n",
    "                all_msg.append(message)\n",
    "                \n",
    "            with open(file, 'w',newline='',encoding='utf-8') as f:\n",
    "                writer = csv.writer(f, lineterminator='\\n')\n",
    "                writer.writerow([\"created_at\",\"retweet\", \"user_name\", \"screen_name\", \"message\"])\n",
    "                writer.writerows(msglist)\n",
    "            break\n",
    "                               \n",
    "        except tweepy.TweepError as e:\n",
    "            print(e.reason)\n",
    "            sleep(60*backoff_counter)\n",
    "            backoff_counter += 1\n",
    "            continue\n",
    "        except:\n",
    "            print(\"Unexpected Error\")\n",
    "            sleep(60*backoff_counter)\n",
    "            backoff_counter += 1\n",
    "            continue\n",
    "\n",
    "data_dir = \"data\"\n",
    "keyword1 = \"#%23CaliforniaFires\"  # #CaliforniaFires\n",
    "file1 = data_dir + \"/CaliforniaFires_20181117-20181126.txt\"\n",
    "\n",
    "collect_tweet(keyword1, file1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted tweets contain tweets from 17th November to 26th November.\n",
    "From this data, I selected tweets from 17th Nov to 20th Nov to reduce specific trend of tweets from thanks giving holiday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = data_dir + \"/\" + \"CaliforniaFires_20181117-20181126.txt\"\n",
    "file_out = data_dir + \"/\" + \"CaliforniaFires.txt\"\n",
    "\n",
    "tweets = pd.read_csv(file)   \n",
    "tweets_sub = tweets[tweets.created_at < '2018-11-21 00:00:00']\n",
    "tweets_sub.to_csv(file_out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adding \"Bot Probability\" \n",
    "After we retrieve tweets from hashtag #CaliforniaFires, we assigned probability of being Bots for each tweets using 'Botmeter'. \n",
    "Since calling this API takes long time, randome sample was taken from CaliforniaFires.txt\n",
    "\n",
    "In this research, I took two randome samples.\n",
    "\n",
    "1) 2000 tweets were randomely taken from CaliforniaFires.txt and probability of 1912 accounts were assigned via Botmeter\n",
    "\n",
    "2) 4000 samples were randomely taken from CaliforniaFires.txt and probability of 3686 \n",
    "accounts were assigned via Botmeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'cap'\n",
      "KeyError-screen name:SistaM_satx\n",
      "'cap'\n",
      "KeyError-screen name:RostislavVasko\n"
     ]
    }
   ],
   "source": [
    "def evaluate_bot(user_list):\n",
    "    \"\"\"\n",
    "    Evaluate probability of user account's being Bot\n",
    "        \n",
    "    Parameters\n",
    "    --------------\n",
    "    user_list\n",
    "     A list of screen name\n",
    "        \n",
    "    Returns\n",
    "    --------------\n",
    "     A list of screen name, probability of being Bot \n",
    "        \n",
    "    \"\"\"    \n",
    "    bot_prob_list = []\n",
    "    \n",
    "    twitter_app_auth = {\n",
    "        'consumer_key': consumer_key,\n",
    "        'consumer_secret': consumer_secret,\n",
    "        'access_token': access_token,\n",
    "        'access_token_secret': access_token_secret,\n",
    "      }\n",
    "    \n",
    "    #Call Botometer\n",
    "    bom = botometer.Botometer(wait_on_ratelimit=True,\n",
    "                              mashape_key=mashape_key,\n",
    "                              **twitter_app_auth)\n",
    "    try:\n",
    "        i = 1\n",
    "        for screen_name, result in bom.check_accounts_in(user_list):\n",
    "            try:\n",
    "                cap_en = result[\"cap\"][\"english\"]\n",
    "                cap_unv = result[\"cap\"][\"universal\"]\n",
    "                bot_prob = { \"screen_name\": screen_name, \"cap_en\": cap_en, \"cap_unv\": cap_unv }\n",
    "                bot_prob_list.append(bot_prob)\n",
    "                \n",
    "                i = i + 1\n",
    "            except KeyError as ke:\n",
    "                print(ke)\n",
    "                print(\"KeyError-screen name:\" + screen_name)\n",
    "            except ConnectionError as ce:\n",
    "                print(ce)\n",
    "                print(\"ConnectionError-screen name:\" + screen_name)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Exception-screen name:\" + screen_name)\n",
    "            except:\n",
    "                print(\"Unexpected-screen name:\" + screen_name)               \n",
    "    except:\n",
    "        print(\"Unexpected(check_accounts)-screen name:\" + screen_name)\n",
    "        print(result)             \n",
    "    \n",
    "    return bot_prob_list\n",
    "\n",
    "\n",
    "#Read tweet list\n",
    "data_dir = \"data\"\n",
    "file = data_dir + \"/\" + \"CaliforniaFires.txt\"\n",
    "file_out = data_dir + \"/\" + \"CaliforniaFires_BotProb.csv\"\n",
    "\n",
    "tweets = pd.read_csv(file)    \n",
    "tweets = tweets.drop(\"created_at\", axis=1)\n",
    "tweets = tweets.drop_duplicates()\n",
    "\n",
    "#Radom sampling from CaliforniaFires.txt\n",
    "tweets_sample = tweets.sample(n=4000)\n",
    "\n",
    "#Extract user list from the sample\n",
    "user_list = list(tweets_sample[\"screen_name\"].unique())\n",
    "\n",
    "user_list_tmp = user_list\n",
    "while(True):\n",
    "    #Consider to recall evaluate_bot in case error is raised\n",
    "    prob_list = evaluate_bot(user_list_tmp)\n",
    "    last_name = prob_list[-1][\"screen_name\"]\n",
    "    if last_name == user_list[-1] : #if all user was evaluated\n",
    "        break\n",
    "    last_index = user_list_tmp.index(last_name)\n",
    "    #create list from the next to the last user in prob_list \n",
    "    user_list_tmp = user_list_tmp[last_index+1:]\n",
    "\n",
    "#Write bot probability\n",
    "with open(file_out, 'w',newline='',encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"screen_name\", \"cap_en\", \"cap_unv\"])\n",
    "    writer.writeheader()\n",
    "    for prob in prob_list:\n",
    "        writer.writerow(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Variable creation and file generation for R\n",
    "\n",
    "### 5.1. Add bot probability to tweets\n",
    "I joined the tweet file 'CaliforniaFires.txt' with Bot probability file CaliforniaFires_BotProb.csv' by screen name.  \n",
    "Now, Bot probability was added to each tweet.\n",
    "\n",
    "### 5.2. Remove duplicated tweets\n",
    "I removed duplicated tweets which have same user name and same message.\n",
    "\n",
    "### 5.3. Add secondary data source to tweets\n",
    "Two files are created in advance of this data processing.\n",
    "\n",
    "1) policalwords.txt\n",
    "\n",
    "This file contain 100 words related to political scandal.  \n",
    "This word list is created from  \n",
    "https://www.vocabulary.com/lists/183710.  \n",
    "This data is used to define variable 'num_political_word'\n",
    "\n",
    "2) environmentalwords.txt\n",
    "\n",
    "This file contains 123 words related to environment.  \n",
    "This word list is created from  \n",
    "https://www.englisch-hilfen.de/en/words/environment.htm  \n",
    "This data is used to define variable 'num_environmental_word'\n",
    "\n",
    "### 5.4. Add variables\n",
    "\n",
    "Below variables were defined.\n",
    "\n",
    "\n",
    "|variable   | description |\n",
    "|:-------|----------|\n",
    "|retweet | How many times the tweets were retweeted by other users |\n",
    "|user_name| Twitter user name  |\n",
    "|screen_name| Twitter screen name  |\n",
    "|message  | Tweeted message  |\n",
    "|bot_probability  | Probability of being Bot. This value is taken from CAP(Complete Automation Probability) returned from Botometer. |\n",
    "|num_word  | Number of words in the message  |\n",
    "|num_question  | Number of question mark '?' in the message  |\n",
    "|num_exclamation  | Number of exclamation mark '!' in the message  |\n",
    "|num_digit_screen_name  | Number of digit(0-9) in user screen name  |\n",
    "|num_political_word  | Number of political word in the mssage|\n",
    "|num_environmental_word  | Number of environmental word in the mssage|\n",
    "|include_retweet  | count 1 if the message is retweet of other tweet|\n",
    "|num_hashtag  | Number of hashtag in the mssage|\n",
    "\n",
    "### 5.5. Generate dataset for R\n",
    "Data set was exported as 'CaliforniaFires_Tweet_Stats.csv' for statistical analysis in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation from sentences\n",
    "def depunctify(sentences):\n",
    "    sentences = sentences.replace(\"--\", \" \")\n",
    "    punct = string.punctuation + \"‘’”“\"\n",
    "    for p in punct:\n",
    "        sentences = sentences.replace(p, \"\")\n",
    "    return sentences    \n",
    "\n",
    "#Count number of words in sentences\n",
    "def count_num_words(row):\n",
    "    sentence = depunctify(row.message)\n",
    "    sentence = sentence.replace(\"\\n\", \" \")\n",
    "    return len(sentence.split())\n",
    "\n",
    "#Count number of question mark\n",
    "def count_num_question(row):\n",
    "    return row.message.count(\"?\")\n",
    "\n",
    "#Count number of exclamation mark\n",
    "def count_num_exclamation(row):\n",
    "    return row.message.count(\"!\")\n",
    "\n",
    "#Count number of digits in screen name\n",
    "def count_num_digit(row):\n",
    "    return sum(c.isdigit() for c in row.screen_name) \n",
    "\n",
    "#Count number of hashtag in tweet\n",
    "def count_num_hashtag(row):\n",
    "    return row.message.count(\"#\")\n",
    "\n",
    "#Count number of specific words\n",
    "def count_num_specific_words(row, df_word):\n",
    "    cnt = 0\n",
    "    msg = row.message\n",
    "    msg = msg.lower()\n",
    "    word_list = msg.split()\n",
    "    \n",
    "    for i in range(len(df_word)):\n",
    "        cnt += df_word.iloc[i,0] in word_list\n",
    "    return cnt\n",
    "\n",
    "#Decide whether message include 'Global Warming'\n",
    "def include_word(row, word):\n",
    "    msg = row.message\n",
    "    msg = msg.lower()\n",
    "    word = word.lower()\n",
    "    idx = msg.find(word)\n",
    "    if idx > -1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data_dir = \"data\"\n",
    "tweet_file = data_dir + \"/CaliforniaFires.txt\"\n",
    "botprob_file = data_dir + \"/CaliforniaFires_BotProb.csv\"\n",
    "political_file = data_dir + \"/policalwords.txt\"\n",
    "environmental_file = data_dir + \"/environmentalwords.txt\"\n",
    "tweet_stats_file = data_dir + \"/CaliforniaFires_Tweet_Stats.csv\"\n",
    "\n",
    "tweet = pd.read_csv(tweet_file)\n",
    "prob = pd.read_csv(botprob_file) \n",
    "political_words = pd.read_table(political_file, names=[\"word\"])\n",
    "env_words = pd.read_table(environmental_file, names=[\"word\"])\n",
    "\n",
    "#Join two dataframes by screen name\n",
    "tweet_df = pd.merge(tweet, prob, on=\"screen_name\")\n",
    "\n",
    "tweet_df = tweet_df.drop(\"cap_unv\", axis=1)\n",
    "tweet_df = tweet_df.drop(\"created_at\", axis=1)\n",
    "\n",
    "tweet_df = tweet_df.rename(columns = {\"cap_en\": \"bot_probability\"})\n",
    "tweet_df = tweet_df.drop_duplicates()\n",
    "tweet_df = tweet_df.sort_values([\"screen_name\",\"message\",\"retweet\"], ascending=[True, True, False])\n",
    "tweet_df.loc[:, \"row_num\"] = tweet_df.groupby([\"screen_name\", \"message\"]).cumcount()\n",
    "tweet_df = tweet_df[tweet_df.row_num == 0]\n",
    "\n",
    "tweet_df.loc[:, \"num_words\"] = tweet_df.apply(count_num_words, axis=1)\n",
    "tweet_df.loc[:, \"num_question\"] = tweet_df.apply(count_num_question, axis=1)\n",
    "tweet_df.loc[:, \"num_exclamation\"] = tweet_df.apply(count_num_exclamation, axis=1)\n",
    "tweet_df.loc[:, \"num_digit_screen_name\"] = tweet_df.apply(count_num_digit, axis=1)\n",
    "tweet_df.loc[:, \"num_political_word\"] = tweet_df.apply(count_num_specific_words, df_word=political_words, axis=1)\n",
    "tweet_df.loc[:, \"num_environmental_word\"] = tweet_df.apply(count_num_specific_words, df_word=env_words, axis=1)\n",
    "tweet_df.loc[:, \"include_retweet\"] = tweet_df.apply(include_word, word=\"RT\", axis=1)\n",
    "tweet_df.loc[:, \"num_hashtag\"] = tweet_df.apply(count_num_hashtag, axis=1)\n",
    "\n",
    "\n",
    "tweet_df.to_csv(tweet_stats_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reference\n",
    "https://www.usnews.com/news/top-news/articles/2018-11-25/number-of-missing-in-deadly-california-wildfire-revised-down-more-rain-on-the-way  \n",
    "\n",
    "https://botometer.iuni.iu.edu/#!/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
